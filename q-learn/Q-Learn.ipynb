{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57a6ec22",
   "metadata": {},
   "source": [
    "# Aprendizaje por Refuerzo - Q-Learning\n",
    "\n",
    "La mayoría de ustedes probablemente han oído hablar de que la inteligencia artificial aprende a jugar juegos de computadora por su cuenta, un ejemplo muy popular es Deepmind. Deepmind llegó a las noticias cuando su programa AlphaGo derrotó al campeón mundial de Go de Corea del Sur en 2016. Hubo muchos intentos exitosos en el pasado para desarrollar agentes con la intención de jugar juegos de Atari como Breakout, Pong y Space Invaders.\n",
    "\n",
    "Cada uno de estos programas sigue un paradigma de aprendizaje automático conocido como **aprendizaje por refuerzo**. Si nunca antes ha estado expuesto al aprendizaje por refuerzo, la siguiente es una analogía muy sencilla de cómo funciona."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869c2c80",
   "metadata": {},
   "source": [
    "## Analogía del Aprendizaje por Refuerzo\n",
    "\n",
    "Considere el escenario de enseñarle nuevos trucos a un perro. El perro no entiende nuestro idioma, por lo que no podemos decirle qué hacer. En cambio, seguimos una estrategia diferente. Emulamos una situación (o una señal) y el perro intenta responder de muchas formas diferentes. Si la respuesta del perro es la deseada, lo premiamos con bocadillos. Ahora adivinen qué, la próxima vez que el perro se exponga a la misma situación, el perro ejecuta una acción similar con aún más entusiasmo a la espera de más comida. Eso es como aprender \"qué hacer\" a partir de experiencias positivas. Del mismo modo, los perros tenderán a aprender qué no hacer cuando se enfrenten a experiencias negativas.\n",
    "\n",
    "Así es exactamente como funciona el aprendizaje por refuerzo en un sentido más amplio:\n",
    "\n",
    "* Su perro es un \"agente\" que está expuesto al **entorno**. El entorno podría estar en tu casa, contigo.\n",
    "* Las situaciones que encuentran son análogas a un **estado**. Un ejemplo de un estado podría ser tu perro parado y usas una palabra específica en un tono determinado en tu sala de estar.\n",
    "* Nuestros agentes reaccionan realizando una **acción** para pasar de un \"estado\" a otro \"estado\", por ejemplo, su perro pasa de estar de pie a sentado.\n",
    "* Después de la transición, pueden recibir una **recompensa** o una **penalización** a cambio. ¡Les das un capricho! O un \"No\" como penalización.\n",
    "* La **política** es la estrategia de elegir una acción en un estado determinado con la expectativa de mejores resultados.\n",
    "\n",
    "El aprendizaje por refuerzo se encuentra entre el espectro del aprendizaje supervisado y el aprendizaje no supervisado, y hay algunas cosas importantes a tener en cuenta:\n",
    "\n",
    "1. **Ser codicioso no siempre funciona**\n",
    "    Hay cosas que son fáciles de hacer para obtener una gratificación instantánea, y hay cosas que brindan recompensas a largo plazo. El objetivo es no ser codicioso buscando recompensas inmediatas rápidas, sino optimizar para obtener las máximas recompensas durante todo el entrenamiento.\n",
    "2. **La secuencia importa en el aprendizaje por refuerzo**\n",
    "    El agente de recompensa no solo depende del estado actual, sino de toda la historia de los estados. A diferencia del aprendizaje supervisado y no supervisado, el tiempo es importante aquí."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13aad911",
   "metadata": {},
   "source": [
    "## Proceso de Aprendizaje por Refuerzo\n",
    "\n",
    "![](img/proceso.gif)\n",
    "\n",
    "En cierto modo, el aprendizaje por refuerzo es la ciencia de tomar decisiones óptimas utilizando experiencias. Al desglosarlo, el proceso de aprendizaje por refuerzo incluye estos sencillos pasos:\n",
    "\n",
    "1. Observación del entorno\n",
    "2. Decidir cómo actuar usando alguna estrategia\n",
    "3. Actuando en consecuencia\n",
    "4. Recibir una recompensa o una penalización\n",
    "5. Aprendiendo de las experiencias y perfeccionando nuestra estrategia\n",
    "6. Iterar hasta encontrar una estrategia óptima\n",
    "\n",
    "Ahora entendamos el aprendizaje por refuerzo desarrollando realmente un agente para que aprenda a jugar un juego automáticamente por sí solo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa00c072",
   "metadata": {},
   "source": [
    "## Ejemplo de Diseño: Taxi Autónomo\n",
    "\n",
    "Diseñemos una simulación de un taxi autónomo. El objetivo principal es demostrar, en un entorno simplificado, cómo se pueden utilizar las técnicas de RL para desarrollar un enfoque eficaz y seguro para abordar este problema.\n",
    "\n",
    "El trabajo del *Smartcab* es recoger al pasajero en un lugar y dejarlo en otro. Aquí hay algunas cosas de las que nos encantaría que se ocupara nuestra Smartcab:\n",
    "\n",
    "* Deje al pasajero en el lugar correcto.\n",
    "* Ahorre tiempo al pasajero tomando el mínimo tiempo posible para dejar\n",
    "* Cuida la seguridad y las normas de tráfico de los pasajeros.\n",
    "\n",
    "Hay diferentes aspectos que deben considerarse aquí al modelar una solución de RL para este problema: recompensas, estados y acciones.\n",
    "\n",
    "### 1. Recompensas\n",
    "\n",
    "Dado que el agente (el conductor imaginario) está motivado por las recompensas y va a aprender a controlar el taxi mediante experiencias de prueba en el entorno, debemos decidir las recompensas y/o sanciones y su magnitud en consecuencia. Aquí algunos puntos a considerar:\n",
    "\n",
    "* El agente debe recibir una alta recompensa positiva por una baja exitosa porque este comportamiento es muy deseado.\n",
    "* El agente debe ser penalizado si intenta dejar a un pasajero en lugares incorrectos.\n",
    "* El agente debería obtener una recompensa levemente negativa por no llegar al destino después de cada paso de tiempo. \"Ligeramente\" negativo porque preferiríamos que nuestro agente llegara tarde en lugar de hacer movimientos incorrectos tratando de llegar al destino lo más rápido posible\n",
    "\n",
    "### 2. Espacio de Estado\n",
    "\n",
    "En el aprendizaje por refuerzo, el agente se encuentra con un estado y luego actúa de acuerdo con el estado en el que se encuentra.\n",
    "\n",
    "El **Espacio de Estado** es el conjunto de todas las situaciones posibles en las que podría vivir nuestro taxi. El estado debe contener información útil que el agente necesita para realizar la acción correcta.\n",
    "\n",
    "Digamos que tenemos un área de entrenamiento para nuestro Smartcab donde lo estamos enseñando a transportar personas en un estacionamiento a cuatro ubicaciones diferentes (R, G, Y, B):\n",
    "\n",
    "![](img/taxi.png)\n",
    "\n",
    "Supongamos que el Smartcab es el único vehículo en este estacionamiento. Podemos dividir el estacionamiento en una cuadrícula de 5x5, lo que nos da 25 posibles ubicaciones de taxis. Estas 25 ubicaciones son una parte de nuestro espacio de estado. Observe que el estado de ubicación actual de nuestro taxi es la coordenada (3, 1).\n",
    "\n",
    "También notará que hay cuatro (4) ubicaciones donde podemos recoger y dejar a un pasajero: R, G, Y, B o [(0,0), (0,4), (4,0), (4,3)] en las coordenadas (fila, columna). Nuestro pasajero ilustrado está en la ubicación Y y desea ir a la ubicación R.\n",
    "\n",
    "Cuando también contabilizamos un (1) estado de pasajero adicional de estar dentro del taxi, podemos tomar todas las combinaciones de ubicaciones de pasajeros y ubicaciones de destino para llegar a un número total de estados para nuestro entorno de taxi; hay cuatro (4) destinos y cinco (4 + 1) ubicaciones de pasajeros.\n",
    "\n",
    "Entonces, nuestro entorno de taxis tiene $5×5×5×4=500$ estados posibles totales.\n",
    "\n",
    "### 3. Espacio de Acción\n",
    "\n",
    "El agente se encuentra con uno de los 500 estados y realiza una acción. La acción en nuestro caso puede ser moverse en una dirección o decidir recoger/dejar a un pasajero.\n",
    "\n",
    "En otras palabras, tenemos seis acciones posibles:\n",
    "\n",
    "1. south\n",
    "2. north\n",
    "3. east\n",
    "4. west\n",
    "5. pickup\n",
    "6. dropoff\n",
    "\n",
    "Este es el espacio de acción: el conjunto de todas las acciones que nuestro agente puede realizar en un estado determinado.\n",
    "\n",
    "Notarás en la ilustración de arriba, que el taxi no puede realizar ciertas acciones en ciertos estados debido a las paredes. En el código del entorno, simplemente proporcionaremos una penalización de -1 por cada golpe de pared y el taxi no se moverá a ningún lado. Esto solo acumulará multas y hará que el taxi considere dar la vuelta a la pared."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8e7972",
   "metadata": {},
   "source": [
    "## Implementación con Python\n",
    "\n",
    "Afortunadamente, **OpenAI Gym** tiene este entorno exacto ya construido para nosotros.\n",
    "\n",
    "Gym proporciona diferentes entornos de juego que podemos conectar a nuestro código y probar un agente. La biblioteca se encarga de la API para proporcionar toda la información que nuestro agente requeriría, como posibles acciones, puntaje y estado actual. Solo necesitamos enfocarnos solo en la parte del algoritmo para nuestro agente.\n",
    "\n",
    "Usaremos el entorno Gym llamado `Taxi-V3`, del que se extrajeron todos los detalles explicados anteriormente. Los objetivos, recompensas y acciones son todos iguales.\n",
    "\n",
    "### Interfaz del Gym\n",
    "\n",
    "Primero tenemos que instalar el `gym`. Ejecutar lo siguiente en un cuaderno de Jupyter debería funcionar:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8902b6c2",
   "metadata": {},
   "source": [
    "```bash\n",
    "!pip install cmake 'gym[atari]' scipy\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521beaa1",
   "metadata": {},
   "source": [
    "Una vez instalado, podemos cargar el entorno del juego y renderizar lo que parece:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "264f9c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: |\u001b[43m \u001b[0m: :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"Taxi-v3\").env\n",
    "\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4815ed6",
   "metadata": {},
   "source": [
    "La interfaz principal de gym es `env`, que es la interfaz del entorno unificado. Los siguientes son los métodos env que nos serían de gran ayuda:\n",
    "\n",
    "* `env.reset`: restablece el entorno y devuelve un estado inicial aleatorio.\n",
    "* `env.step` (acción): Paso el entorno en un paso de tiempo. Retorna\n",
    "    + **observation**: Observaciones del entorno\n",
    "    + **reward**: si su acción fue beneficiosa o no\n",
    "    + **done**: Indica si hemos recogido y dejado a un pasajero, también llamado episodio\n",
    "    + **info**: información adicional como rendimiento y latencia para fines de depuración\n",
    "* `env.render`: renderiza un fotograma del entorno (útil para visualizar el entorno)\n",
    "\n",
    "Nota: Estamos usando el `.env` al final de make para evitar que el entrenamiento se detenga en 200 iteraciones, que es el valor predeterminado para la nueva versión de Gym (referencia).\n",
    "\n",
    "### Recordatorio de nuestro problema\n",
    "\n",
    "Aquí está nuestra declaración de problema reestructurada (de los documentos de Gym):\n",
    "\n",
    ">\"*Hay 4 ubicaciones (etiquetadas con letras diferentes), y nuestro trabajo es recoger al pasajero en una ubicación y dejarlo en otra. Recibimos +20 puntos por una entrega exitosa y perdemos 1 punto por cada vez. paso que da. También hay una penalización de 10 puntos por acciones ilegales de recoger y dejar*\".\n",
    "\n",
    "Sumérjase más en el entrono."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcce89e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[43mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n",
      "Action Space Discrete(6)\n",
      "State Space Discrete(500)\n"
     ]
    }
   ],
   "source": [
    "env.reset() # reset environment to a new, random state\n",
    "env.render()\n",
    "\n",
    "print(\"Action Space {}\".format(env.action_space))\n",
    "print(\"State Space {}\".format(env.observation_space))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbd03da",
   "metadata": {},
   "source": [
    "* El **cuadrado relleno** representa el taxi, que es amarillo sin pasajero y verde con pasajero.\n",
    "* **pipe** (\"|\") representa una pared que el taxi no puede cruzar.\n",
    "* **R, G, Y, B** son las posibles ubicaciones de recogida y destino. La letra azul representa la ubicación actual de recogida de pasajeros y la letra morada es el destino actual.\n",
    "\n",
    "Como verifican las impresiones, tenemos un **Action Space** de tamaño 6 y un **State Space** de tamaño 500. Como verá, nuestro algoritmo RL no necesitará más información que estas dos cosas. Todo lo que necesitamos es una forma de identificar un estado de forma única asignando un número único a cada estado posible, y RL aprende a elegir un número de acción de 0 a 5 donde:\n",
    "\n",
    "* 0 = south\n",
    "* 1 = north\n",
    "* 2 = east\n",
    "* 3 = west\n",
    "* 4 = pickup\n",
    "* 5 = dropoff\n",
    "\n",
    "Recuerde que los 500 estados corresponden a una codificación de la ubicación del taxi, la ubicación del pasajero y la ubicación de destino.\n",
    "\n",
    "El aprendizaje por refuerzo aprenderá un mapeo de **estados** para la **acción** óptima a realizar en ese estado mediante la exploración, es decir, el agente explora el entorno y toma acciones basadas en las recompensas definidas en el entorno.\n",
    "\n",
    "La acción óptima para cada estado es la acción que tiene la **mayor recompensa acumulativa a largo plazo**.\n",
    "\n",
    "### Volver a nuestra ilustración\n",
    "\n",
    "De hecho, podemos tomar nuestra ilustración anterior, codificar su estado y dárselo al entorno para que se renderice en Gym. Recuerde que tenemos el taxi en la fila 3, columna 1, nuestro pasajero está en la ubicación 2 y nuestro destino es la ubicación 0. Usando el método de codificación de estado Taxi-v3, podemos hacer lo siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44f7e0cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: 328\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "state = env.encode(3, 1, 2, 0) # (taxi row, taxi column, passenger index, destination index)\n",
    "print(\"State:\", state)\n",
    "\n",
    "env.s = state\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3e2619",
   "metadata": {},
   "source": [
    "Estamos usando las coordenadas de nuestra ilustración para generar un número correspondiente a un estado entre 0 y 499, que resulta ser **328** para el estado de nuestra ilustración.\n",
    "\n",
    "Luego, podemos establecer el estado del entorno manualmente con `env.env.s` usando ese número codificado. Puede jugar con los números y verá que el taxi, el pasajero y el destino se mueven.\n",
    "\n",
    "### La Tabla de Recompensas\n",
    "\n",
    "Cuando se crea el entorno de Taxi, también se crea una tabla de recompensas inicial, llamada \"P\". Podemos pensar en ello como una matriz que tiene el número de estados como filas y el número de acciones como columnas, es decir, una matriz $states × actions$.\n",
    "\n",
    "Dado que cada estado está en esta matriz, podemos ver los valores de recompensa predeterminados asignados al estado de nuestra ilustración:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb8966d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [(1.0, 428, -1, False)],\n",
       " 1: [(1.0, 228, -1, False)],\n",
       " 2: [(1.0, 348, -1, False)],\n",
       " 3: [(1.0, 328, -1, False)],\n",
       " 4: [(1.0, 328, -10, False)],\n",
       " 5: [(1.0, 328, -10, False)]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.P[328]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14486e8e",
   "metadata": {},
   "source": [
    "Este diccionario tiene la estructura `{action: [(probability, nextstate, reward, done)]}`.\n",
    "\n",
    "Algunas cosas a tener en cuenta:\n",
    "\n",
    "* Del 0-5 corresponde a las acciones (sur, norte, este, oeste, recogida, bajada) que el taxi puede realizar en nuestro estado actual en la ilustración.\n",
    "* En este entorno, la probabilidad siempre es 1.0.\n",
    "* El siguiente estado es el estado en el que estaríamos si tomamos la acción en este índice del dict\n",
    "* Todas las acciones de movimiento tienen una recompensa de -1 y las acciones de recoger/dejar tienen una recompensa de -10 en este estado en particular. Si estamos en un estado en el que el taxi tiene un pasajero y está en la parte superior del destino correcto, veríamos una recompensa de 20 en la acción de devolución (5)\n",
    "* `done` se utiliza para indicarnos cuándo hemos dejado a un pasajero en el lugar correcto. Cada abandono exitoso es el final de un episodio.\n",
    "\n",
    "Tenga en cuenta que si nuestro agente eligiera explorar la acción dos (2) en este estado, iría hacia el este y chocaría contra una pared. El código fuente ha hecho imposible mover el taxi a través de una pared, por lo que si el taxi elige esa acción, seguirá acumulando -1 penalizaciones, lo que afecta la recompensa a largo plazo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de4a54c",
   "metadata": {},
   "source": [
    "## Resolviendo el Entorno sin Aprendizaje por Refuerzo\n",
    "\n",
    "Veamos qué pasaría si intentamos utilizar la fuerza bruta para resolver el problema sin RL.\n",
    "\n",
    "Dado que tenemos nuestra tabla P para las recompensas predeterminadas en cada estado, podemos intentar que nuestro taxi navegue solo con eso.\n",
    "\n",
    "Crearemos un bucle infinito que se ejecutará hasta que un pasajero llegue a un destino (un episodio), o en otras palabras, cuando la recompensa recibida sea 20. El método `env.action_space.sample()` selecciona automáticamente una acción aleatoria del conjunto de todos posibles acciones.\n",
    "\n",
    "Veamos qué pasa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0ff5de8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timesteps taken: 845\n",
      "Penalties incurred: 272\n"
     ]
    }
   ],
   "source": [
    "env.s = 328  # set environment to illustration's state\n",
    "\n",
    "epochs = 0\n",
    "penalties, reward = 0, 0\n",
    "\n",
    "frames = [] # for animation\n",
    "\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, info = env.step(action)\n",
    "\n",
    "    if reward == -10:\n",
    "        penalties += 1\n",
    "    \n",
    "    # Put each rendered frame into dict for animation\n",
    "    frames.append({\n",
    "        'frame': env.render(mode='ansi'),\n",
    "        'state': state,\n",
    "        'action': action,\n",
    "        'reward': reward\n",
    "        }\n",
    "    )\n",
    "\n",
    "    epochs += 1\n",
    "    \n",
    "    \n",
    "print(\"Timesteps taken: {}\".format(epochs))\n",
    "print(\"Penalties incurred: {}\".format(penalties))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b890d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35m\u001b[34;1m\u001b[43mR\u001b[0m\u001b[0m\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "\n",
      "Timestep: 845\n",
      "State: 0\n",
      "Action: 5\n",
      "Reward: 20\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "\n",
    "def print_frames(frames):\n",
    "    for i, frame in enumerate(frames):\n",
    "        clear_output(wait=True)\n",
    "        print(frame['frame'])\n",
    "        print(f\"Timestep: {i + 1}\")\n",
    "        print(f\"State: {frame['state']}\")\n",
    "        print(f\"Action: {frame['action']}\")\n",
    "        print(f\"Reward: {frame['reward']}\")\n",
    "        sleep(.1)\n",
    "        \n",
    "print_frames(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71897ffe",
   "metadata": {},
   "source": [
    "No es bueno. Nuestro agente toma miles de pasos de tiempo y realiza muchas devoluciones incorrectas para entregar un solo pasajero al destino correcto.\n",
    "\n",
    "Esto se debe a que no estamos aprendiendo de la experiencia pasada. Podemos ejecutar esto una y otra vez, y nunca se optimizará. El agente no tiene memoria de qué acción fue la mejor para cada estado, que es exactamente lo que hará por nosotros el Aprendizaje por refuerzo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009c5394",
   "metadata": {},
   "source": [
    "## Ingrese al Aprendizaje por Refuerzo\n",
    "\n",
    "Vamos a utilizar un algoritmo RL simple llamado Q-learning que le dará a nuestro agente algo de memoria.\n",
    "\n",
    "### Introducción a Q-learning\n",
    "\n",
    "Básicamente, Q-learning permite al agente utilizar las recompensas del entorno para aprender, con el tiempo, la mejor acción a realizar en un estado determinado.\n",
    "\n",
    "En nuestro entorno de Taxi, tenemos la tabla de recompensas, `P`, de la que el agente aprenderá. Lo hace buscando recibir una recompensa por realizar una acción en el estado actual y luego actualizar un valor Q para recordar si esa acción fue beneficiosa\n",
    "\n",
    "Los valores almacenados en tabla-Q (Q-table) se denominan valores-Q (Q-values) y se asignan a una combinación `(estado, acción)`.\n",
    "\n",
    "Un valor-Q para una combinación de acción de estado particular es representativo de la \"calidad\" de una acción tomada desde ese estado. Mejores valores-Q implican mejores posibilidades de obtener mayores recompensas.\n",
    "\n",
    "Por ejemplo, si el taxi se enfrenta a un estado que incluye a un pasajero en su ubicación actual, es muy probable que el valor-Q para `pickup` sea más alto en comparación con otras acciones, como `dropoff` o `north`.\n",
    "\n",
    "Los valores-Q se inicializan a un valor arbitrario y, a medida que el agente se expone al entorno y recibe diferentes recompensas al ejecutar diferentes acciones, los valores-Q se actualizan mediante la ecuación:\n",
    "\n",
    "$$\n",
    "Q(state,action) \\leftarrow (1 - \\alpha)Q(state,action) + \\alpha (reward + \\gamma \\max _{a}Q(next state,all actions)\n",
    "$$\n",
    "\n",
    "Dónde:\n",
    "\n",
    "* $\\alpha$ (alfa) es la tasa de aprendizaje ($0 \\lt \\alpha \\le 1$) - Al igual que en los entornos de aprendizaje supervisado, $\\alpha$ es la medida en que nuestros valores Q se actualizan en cada iteración.\n",
    "* $\\gamma$ (gamma) es el factor de descuento ($0 \\le \\gamma \\le 1$): determina la importancia que queremos dar a las recompensas futuras. Un valor alto para el factor de descuento (cercano a **1**) captura la recompensa efectiva a largo plazo, mientras que un factor de descuento de **0** hace que nuestro agente considere solo la recompensa inmediata, lo que lo vuelve codicioso.\n",
    "\n",
    "**¿Qué está diciendo esto?**\n",
    "\n",
    "Estamos asignando ($\\leftarrow$), o actualizando, el valor Q del estado actual del agente y la acción tomando primero un peso ($1 - \\alpha$) del antiguo valor Q, luego agregando el valor aprendido. El valor aprendido es una combinación de la recompensa por realizar la acción actual en el estado actual y la recompensa máxima descontada del siguiente estado en el que estaremos una vez que realicemos la acción actual.\n",
    "\n",
    "Básicamente, estamos aprendiendo la acción adecuada a tomar en el estado actual al observar la recompensa por el combo estado/acción actual y las recompensas máximas para el siguiente estado. Esto eventualmente hará que nuestro taxi considere la ruta con las mejores recompensas juntas.\n",
    "\n",
    "El valor-Q de un par estado-acción es la suma de la recompensa instantánea y la recompensa futura descontada (del estado resultante). La forma en que almacenamos los valores-Q para cada estado y acción es a través de una **tabla-Q**\n",
    "\n",
    "### Tabla-Q\n",
    "\n",
    "La tabla-Q es una matriz donde tenemos una fila para cada estado (500) y una columna para cada acción (6). Primero se inicializa a 0 y luego los valores se actualizan después del entrenamiento. Tenga en cuenta que la tabla-Q tiene las mismas dimensiones que la tabla de recompensas, pero tiene un propósito completamente diferente.\n",
    "\n",
    "![](img/matriz.png)\n",
    "<center>Los valores de Q-Table se inicializan a cero y luego se actualizan durante el entrenamiento a valores que optimizan el recorrido del agente por el entorno para obtener las máximas recompensas.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f0c10d",
   "metadata": {},
   "source": [
    "## Resumiendo el proceso de Q-Learning\n",
    "\n",
    "Rompiéndolo en pasos, obtenemos\n",
    "\n",
    "* Inicialice la tabla-Q en ceros.\n",
    "* Empiece a explorar acciones: para cada estado, seleccione cualquiera de las posibles acciones para el estado actual (S).\n",
    "* Viaje al siguiente estado (S') como resultado de esa acción (a).\n",
    "* Para todas las acciones posibles del estado (S'), seleccione la que tenga el valor Q más alto.\n",
    "* Actualice los valores de la tabla-Q utilizando la ecuación.\n",
    "* Establezca el siguiente estado como el estado actual.\n",
    "* Si se alcanza el estado objetivo, finalice y repita el proceso.\n",
    "\n",
    "### Explotación de valores aprendidos\n",
    "\n",
    "Después de una suficiente exploración aleatoria de acciones, los valores-Q tienden a converger sirviendo a nuestro agente como una función de valor de acción que puede explotar para elegir la acción más óptima de un estado dado.\n",
    "\n",
    "Existe una compensación entre exploración (elegir una acción aleatoria) y explotación (elegir acciones basadas en valores-Q ya aprendidos). Queremos evitar que la acción siga siempre la misma ruta y posiblemente se sobreajuste, por lo que introduciremos otro parámetro llamado $\\epsilon$ \"épsilon\" para atender esto durante el entrenamiento.\n",
    "\n",
    "En lugar de simplemente seleccionar la acción de valor-Q mejor aprendida, a veces preferimos explorar más el espacio de acción. Un valor de épsilon más bajo da como resultado episodios con más penalizaciones (en promedio), lo cual es obvio porque estamos explorando y tomando decisiones al azar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c94394",
   "metadata": {},
   "source": [
    "## Implementando Q-learning en Python\n",
    "\n",
    "### Entrenando al Agente\n",
    "\n",
    "Primero, inicializaremos a cero la tabla-Q a una matriz de $500×6$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef31b8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "q_table = np.zeros([env.observation_space.n, env.action_space.n])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1e8c31",
   "metadata": {},
   "source": [
    "Ahora podemos crear el algoritmo de entrenamiento que actualizará esta tabla-Q a medida que el agente explora el entorno durante miles de episodios.\n",
    "\n",
    "En la primera parte de `while not done`, decidimos si elegir una acción aleatoria o explotar los valores-Q ya calculados. Esto se hace simplemente usando el valor `épsilon` y comparándolo con la función `random.uniform(0, 1)`, que devuelve un número arbitrario entre 0 y 1.\n",
    "\n",
    "Ejecutamos la acción elegida en el entorno para obtener el `next_state` y la `recompensa` por realizar la acción. Después de eso, calculamos el máximo valor-Q para las acciones correspondientes al `next_state`, y con eso, podemos actualizar fácilmente nuestro valor-Q al `new_q_value`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d80b1717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 100000\n",
      "Training finished.\n",
      "\n",
      "CPU times: user 1min 16s, sys: 21.6 s, total: 1min 37s\n",
      "Wall time: 1min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\"\"\"Training the agent\"\"\"\n",
    "\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.1\n",
    "gamma = 0.6\n",
    "epsilon = 0.1\n",
    "\n",
    "# For plotting metrics\n",
    "all_epochs = []\n",
    "all_penalties = []\n",
    "\n",
    "for i in range(1, 100001):\n",
    "    state = env.reset()\n",
    "\n",
    "    epochs, penalties, reward, = 0, 0, 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample() # Explore action space\n",
    "        else:\n",
    "            action = np.argmax(q_table[state]) # Exploit learned values\n",
    "\n",
    "        next_state, reward, done, info = env.step(action) \n",
    "        \n",
    "        old_value = q_table[state, action]\n",
    "        next_max = np.max(q_table[next_state])\n",
    "        \n",
    "        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "        q_table[state, action] = new_value\n",
    "\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        state = next_state\n",
    "        epochs += 1\n",
    "        \n",
    "    if i % 100 == 0:\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Episode: {i}\")\n",
    "\n",
    "print(\"Training finished.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd4cf1d",
   "metadata": {},
   "source": [
    "Ahora que la tabla-Q se ha establecido en más de 100,000 episodios, veamos cuáles son los valores-Q en el estado de nuestra ilustración:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fda19632",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -2.40656838,  -2.27325184,  -2.4050809 ,  -2.35795119,\n",
       "       -10.95270031, -10.67112013])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table[328]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae27952",
   "metadata": {},
   "source": [
    "El max valor-Q es \"north\" (-2.273), por lo que parece que Q-learning ha aprendido efectivamente la mejor acción a realizar en el estado de nuestra ilustración."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2891f988",
   "metadata": {},
   "source": [
    "### Evaluando al Agente\n",
    "\n",
    "Evaluemos el desempeño de nuestro agente. No necesitamos explorar más acciones, por lo que ahora la siguiente acción siempre se selecciona utilizando el mejor valor-Q:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e152ffe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results after 100 episodes:\n",
      "Average timesteps per episode: 13.33\n",
      "Average penalties per episode: 0.0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Evaluate agent's performance after Q-learning\"\"\"\n",
    "\n",
    "total_epochs, total_penalties = 0, 0\n",
    "episodes = 100\n",
    "\n",
    "for _ in range(episodes):\n",
    "    state = env.reset()\n",
    "    epochs, penalties, reward = 0, 0, 0\n",
    "    \n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = np.argmax(q_table[state])\n",
    "        state, reward, done, info = env.step(action)\n",
    "\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        epochs += 1\n",
    "\n",
    "    total_penalties += penalties\n",
    "    total_epochs += epochs\n",
    "\n",
    "print(f\"Results after {episodes} episodes:\")\n",
    "print(f\"Average timesteps per episode: {total_epochs / episodes}\")\n",
    "print(f\"Average penalties per episode: {total_penalties / episodes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3449ad",
   "metadata": {},
   "source": [
    "Podemos ver en la evaluación que el desempeño del agente mejoró significativamente y no incurrió en penalizaciones, lo que significa que realizó las acciones correctas de recogida/devolución con 100 pasajeros diferentes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca95d414",
   "metadata": {},
   "source": [
    "### Comparando nuestro Agente Q-learning con ningún aprendizaje reforzado\n",
    "\n",
    "Con Q-learning, el agente comete errores inicialmente durante la exploración, pero una vez que ha explorado lo suficiente (visto la mayoría de los estados), puede actuar sabiamente maximizando las recompensas haciendo movimientos inteligentes. Veamos cuánto mejor es nuestra solución de Q-learning en comparación con el agente que realiza movimientos aleatorios.\n",
    "\n",
    "Evaluamos a nuestros agentes de acuerdo con las siguientes métricas,\n",
    "\n",
    "* **Número medio de penalizaciones por episodio**: cuanto menor sea el número, mejor será el rendimiento de nuestro agente. Idealmente, nos gustaría que esta métrica fuera cero o muy cercana a cero.\n",
    "* **Cantidad promedio de pasos de tiempo por viaje**: también queremos una pequeña cantidad de pasos de tiempo por episodio, ya que queremos que nuestro agente dé pasos mínimos (es decir, el camino más corto) para llegar al destino.\n",
    "* **Recompensas promedio por movimiento**: cuanto mayor sea la recompensa, significa que el agente está haciendo lo correcto. Es por eso que decidir las recompensas es una parte crucial del aprendizaje por refuerzo. En nuestro caso, dado que tanto los tiempos como las penalizaciones se recompensan negativamente, una recompensa promedio más alta significaría que el agente llega al destino lo más rápido posible con la menor cantidad de penalizaciones\".\n",
    "\n",
    "<img src=\"img/table.png\" width=\"600\">\n",
    "\n",
    "Estas métricas se calcularon en más de 100 episodios. Y como muestran los resultados, ¡nuestro agente de Q-learning lo logró!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682d4081",
   "metadata": {},
   "source": [
    "### Hiperparámetros y Optimizaciones\n",
    "\n",
    "Los valores de `alpha`, `gamma` y `epsilon` se basaron principalmente en la intuición y en algunos \"hit and trial\", pero hay mejores formas de obtener buenos valores.\n",
    "\n",
    "Idealmente, los tres deberían disminuir con el tiempo porque a medida que el agente continúa aprendiendo, en realidad construye antecedentes más resistentes;\n",
    "\n",
    "\n",
    "* $\\alpha$: (la tasa de aprendizaje) debería disminuir a medida que continúe adquiriendo una base de conocimientos cada vez mayor.\n",
    "\n",
    "* $\\gamma$: a medida que se acerca más y más a la terminación, su preferencia por la recompensa a corto plazo debería aumentar, ya que no estará el tiempo suficiente para obtener la recompensa a largo plazo, lo que significa que su gamma debería disminuir.\n",
    "\n",
    "* $\\epsilon$: a medida que desarrollamos nuestra estrategia, tenemos menos necesidad de exploración y más explotación para obtener más utilidad de nuestra política, por lo que a medida que aumentan los ensayos, épsilon debería disminuir.\n",
    "\n",
    "**Ajuste de los hiperparámetros**\n",
    "\n",
    "Una forma sencilla de generar mediante programación el mejor conjunto de valores del hiperparámetro es crear una función de búsqueda integral (similar a la búsqueda en cuadrícula) que seleccione los parámetros que darían como resultado la mejor relación recompensa/pasos_tiempo. El motivo de recompensa/paso_tiempo es que queremos elegir parámetros que nos permitan obtener la máxima recompensa lo más rápido posible. Es posible que también deseemos realizar un seguimiento del número de penalizaciones correspondientes a la combinación de valores de hiperparámetro porque esto también puede ser un factor decisivo (no queremos que nuestro agente inteligente viole las reglas a costa de llegar más rápido). Una forma más elegante de obtener la combinación correcta de valores de hiperparámetros sería usar algoritmos genéticos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1865989",
   "metadata": {},
   "source": [
    "## Conclusión y lo que viene\n",
    "\n",
    "¡Bien! Comenzamos entendiendo el aprendizaje por refuerzo con la ayuda de analogías del mundo real. Luego nos sumergimos en los conceptos básicos del aprendizaje por refuerzo y enmarcamos una taxi autónomo como un problema de aprendizaje por refuerzo. Luego usamos OpenAI's Gym en Python para proporcionarnos un entorno relacionado, donde podemos desarrollar nuestro agente y evaluarlo. Luego observamos lo terrible que era nuestro agente sin usar ningún algoritmo para jugar, así que seguimos adelante para implementar el algoritmo Q-learning desde cero. El rendimiento del agente mejoró significativamente después de Q-learning. Finalmente, discutimos mejores enfoques para decidir los hiperparámetros de nuestro algoritmo.\n",
    "\n",
    "Q-learning es uno de los algoritmos de aprendizaje por refuerzo más fáciles. Sin embargo, el problema con Q-learning es que, una vez que el número de estados en el entorno es muy alto, se vuelve difícil implementarlos con la tabla-Q, ya que el tamaño sería muy, muy grande. Las técnicas de vanguardia utilizan redes neuronales profundas en lugar de tabla-Q (Deep Reinforcement Learning). La red neuronal recibe información de estado y acciones en la capa de entrada y aprende a generar la acción correcta a lo largo del tiempo. Las técnicas de aprendizaje profundo (como las redes neuronales convolucionales) también se utilizan para interpretar los píxeles en la pantalla y extraer información del juego (como puntuaciones), y luego dejar que el agente controle el juego.\n",
    "\n",
    "Hemos hablado mucho sobre el aprendizaje por refuerzo y los juegos. Pero el aprendizaje por refuerzo no se limita solo a los juegos. Se utiliza para administrar carteras de valores y finanzas, para hacer robots humanoides, para la fabricación y la gestión de inventarios, para desarrollar agentes de IA generales, que son agentes que pueden realizar múltiples cosas con un solo algoritmo, como el mismo agente que juega múltiples juegos de Atari. Open AI también tiene una plataforma llamada universo para medir y entrenar la inteligencia general de una IA en miles de juegos, sitios web y otras aplicaciones generales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0019aa88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "softc",
   "language": "python",
   "name": "softc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
